---
title: "Work on large numeric tables with R"
date: "`r Sys.Date()`"
author:
  - name: "BiGR: Bioinformatics core facility of Gustave Roussy"
    email: "bigr@gustaveroussy.fr"
output:
  rmdformatsbigr::readthebigrdoc:  # if you'd like to have a nice theme
    code_folding: show
    thumbnails: false
    lightbox: true
    highlight: true
    fig_caption: false

# install.packages("remotes")
# install.packages("git2r")
# remotes::install_git("https://gitlab.com/bioinfo_gustaveroussy/bigr/rmdformatsbigr.git",
#                      credentials=git2r::cred_user_pass("your_login", "your_password"))
---

```{r setup, include=FALSE}
# options(width = 60);
knitr::opts_chunk$set(
  echo = TRUE,        # Print the code
  eval = TRUE,        # Do not run command lines
  message = FALSE,    # Print messages
  prompt = FALSE,     # Do not display prompt
  comment = NA,       # No comments on this section
  warning = TRUE      # Display warnings
);

```

<style type="text/css">
summary {
  display: list-item;
}
details:hover {
  cursor: pointer
}
body {
  text-align: justify
}
.column-left{
  float: left;
  width: 47%;
  text-align: left;
}
.column-right{
  float: right;
  width: 47%;
  text-align: left;
}
</style>


# Introduction

## Purpose

Loading and filtering large tables is a common task in the fields of Biology and Bioinformatics.
Being able to process all floating point, commas and power of 10 as numbers is a very simple thing
with [`R`](https://en.wikipedia.org/wiki/R_(programming_language)), and other programming languages.

## Expectations

At the end of this session, you will be able to:

1. Load large tables in memory (and save space/time as much as possible)
1. Search any range of values within a numeric table
1. Identify normalized data, raw data
1. Convert TPM to raw counts
1. Normalize raw counts to TPM
1. Sort a numeric table accoring to a criterion of your choice
1. FIlter a numeric table given a range of values
1. Produce and save a PCA
1. Produce and save a clustered heatmap on differentially expressed genes
1. Produce and save a table including rRNA ratio
1. Produce and save a R object with the whole session objects



# Load count table in memory

Loading a table in R is being done with the function [`read.table`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table) 
form the package [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2).
Remember, specifying the package name is facultative, but very
important when you want to share, reproduce, and make your work
accessible. These are the keys of the [FAIR principles](https://fr.wikipedia.org/wiki/Fair_data).

```{r NR_load_table}
counts <- utils::read.table(
    file="EMT_count_data.txt",   # Path to the table
)
```

Let's see the content of the table, using the function [`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from package [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2).

```{r NR_head_loaded_table, eval=FALSE}
utils::head(x = counts)
```

```{r NR_kittr_head_loaded_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=counts),
  caption = "First five lines of the wrongly parsed table"
)
```

Oh, there is a problem here...
Help me identify all our issues here.

<details>

<summary>List of issues</summary>

1. Columns are not separated correctly
1. Header is not recognized
1. Row names are considered as a column

</details>
<br />

<details>

<summary>Solution</summary>

```{r NR_correctly_load_table}
counts <- utils::read.table(
    file="EMT_count_data.txt",
    header=TRUE,
    sep=",",
    row.names=1
)
```

We use the [`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
function again...

```{r NR_head_correctly_loaded_table, eval=FALSE}
utils::head(x = counts)
```

```{r NR_kittr_head_correctly_loaded_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=counts),
  caption = "First five lines of the correctly parsed table"
)
```

It looks just as it shoud!

</details>
<br />

## Conclusion

Always look at your data. Always print the first rows as a
quality control. No exceptions.

# Search gene(s) of interest

## Search gene names in the table

On can access any sample of gene using their name.

Let's search for the gene [`ENO1`](https://www.genecards.org/cgi-bin/carddisp.pl?gene=ENO1):

```{r search_for_eno1}
counts["ENO1", ]
```

Let's focus on the sample `Day_0_R1`:

```{r search_for_eno1_in_dor1}
counts["ENO1", "Day_0_R1"]
```

We can also search for a list of genes:

```{r search_list_genes_all_samples}
counts[c("ENO1", "KRT8"), ]
```

Or within a range of samples:

```{r search_list_genes_range_samples}
counts[
    c("ENO1", "KRT8"), 
    c("Day_0_R1", "Day_0_R2", "Day_0_R3")
]
```

## Search with substrings

We can also use the function [`grep`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/grep)
from package [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2)
to access to all *"Day_1"* samples, with:

```{r grepl_samples, eval=FALSE}
counts[, base::grepl(pattern="Day_1.*", x=base::colnames(counts))]
```


```{r NR_kittr_grepl_samples, echo=FALSE, results='asis'}
knitr::kable(
    x = utils::head(x=counts[, base::grepl(pattern="Day_1.*", x=base::colnames(counts))])
)
```

Or *rRNA* related gene symbols:


```{r grepl_rrna, eval=FALSE}
counts[base::grepl(pattern=".*RRNA.*", x=base::rownames(counts), ignore.case=TRUE), ]
```

```{r NR_kittr_grepl_rrna, echo=FALSE, results='asis'}
knitr::kable(
    x = utils::head(counts[base::grepl(pattern=".*RRNA.*", x=base::rownames(counts)), ])
)
```

[Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) 
is a very, **very** powerful tool. We won't cover it here. I warmly 
invite you to explore further with Regular expressions.

## Conclusion

One can access any range of values within a data frame with:
`data_frame_name[rows, columns]`.

# Transpose, melt, explode, and sort data

Most of the time, you will need to reshape your data.

## Transpose

You may want to invert the row/columns, this is done
with the function [`t`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/t)
from [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/)
package.

```{r transpose_data}
transposed <- base::t(counts)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_transposed_data, eval=FALSE}
utils::head(x=transposed)
```

```{r NR_kittr_head_trnasposed_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=transposed[, 0:5]),
  caption = "Sample identifiers in row, gene names in columns"
)
```

## Melt

We can reduce the number of column while keeping
the exact same amount of data. This reorganisation
of the data is called `melting` data.

We can do it with the function [`melt`](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4/topics/melt)
from the package [`reshape2`](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4).

This is usefull to plot counts according to sample identifiers,
or counts according to experimental conditions, etc.

```{r reshape_melt_data}
base::library(package="reshape2", character.only=TRUE)

# Don't forget to include your row names into
# your data frame, or they will be lost.
named_counts <- counts
named_counts$Gene_Symbol <- row.names(counts)

molten_counts <- reshape2::melt(
    data=named_counts,
    variable.name="Sample_id",
    value.name="Gene_Expression"
)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_reshaped_data, eval=FALSE}
utils::head(x=molten_counts)
```

```{r NR_kittr_head_reshaped_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=molten_counts),
  caption = "First five lines of the molten table"
)
```

## Explode (un-melt)

In the case you recieved molten data, and you'd like
to recover the wide original table. This action is
called `exploding` a table, or sometimes `un-melting`
a table.

We can do it with the function [`cast`](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4/topics/cast)
from the package [`reshape2`](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4/).

```{r dcast_melted_data}
exploded <- reshape2::dcast(
    data=molten_counts,
    formula=Gene_Symbol~Sample_id,
    value.var="Gene_Expression"
)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_exploded_data, eval=FALSE}
utils::head(x=exploded)
```

```{r NR_kittr_head_exploded_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=exploded),
  caption = "First five lines of the exploded table"
)
```

## Sort

We often wand to sort data. Usually, sorting them from the
highest variable genes to the lowest: this help us quickly
see if our genes of interest are variable among our conditions.
It is a quick quality check for any RNASeq experiment.

We are going to apply the function [`var`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor)
from the package [`stats`](https://www.rdocumentation.org/packages/stats/versions/3.6.2),
using the function [`apply`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply)
from the package [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/).

```{r compute_gene_variance}
gene_var <- base::apply(X=counts, MARGIN=1, FUN=stats::var)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_variance_data, eval=FALSE}
utils::head(x=gene_var)
```

```{r NR_kittr_head_variance_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=gene_var),
  caption = "First five lines of the variance table"
)
```

We find the most variable genes with the function [`sort`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sort)
from [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sort) package,
and get their names with the function [`names`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/names)
from the same package.

```{r get_names_of_most_variable_genes}
var_gene_names <- base::names(
    base::sort(
        x=gene_var,
        decreasing=TRUE
    )
)
utils::head(var_gene_names)
```

And now, we apply the sort on our original table:

```{r sort_counts_table}
sorted_counts <- counts[var_gene_names, ]
```


Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_sorted_data, eval=FALSE}
utils::head(x=sorted_counts)
```

```{r NR_kittr_head_sorted_table, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=sorted_counts),
  caption = "First five lines of the sorted count table"
)
```

## Conclusion

There are plenty of other transformations. We will use
these transformed tables through the rest of this session.

# Identify normalized counts

One of the most common question is: are my datasets normalized?

It is not possible to guess it with 100% accuracy. However, there are
many hints. The only way to know if a data has been normalized, is to
ask the data provider.

## Compare sample means and standard deviations

```{r display_summary}
base::summary(counts)
```

We can read the mean expression, alongside with min, max, and standard deviation.
It is easy to plot this with the function [`ggsummarystats`](https://rdrr.io/cran/ggpubr/man/ggsummarystats.html)
from the package [`ggpubr`](https://rdrr.io/cran/ggpubr/).

```{r plot_summary}
base::library(package="ggpubr", character.only=TRUE)
ggpubr::ggsummarystats(
    data=molten_counts,
    x="Sample_id",
    y="Gene_Expression",
    ggfunc = ggboxplot, 
    add = c("median_iqr"),
    summaries=c("mean", "n", "sd"),
    ylim=c(0, 3000)
)
```

Our libraries have very similar means and startad deviation. This is a first
hint: very wide range of means would have signed non-normalized data.

## Visualize counts distribution

We can look at the counts distribution and display it with the functions [`ggplot`](https://ggplot2.tidyverse.org/reference/ggplot.html)
to build the graph, [`aes`](https://ggplot2.tidyverse.org/reference/aes.html) 
to deal with aesthetics, and [`geom_density`](https://ggplot2.tidyverse.org/reference/geom_density.html).
All these functions come from [`ggplot2`](https://ggplot2.tidyverse.org/index.html).

```{r plot_count_density_per_sample}
base::library(package="ggplot2", character.only=TRUE)
ggplot2::ggplot(
    data=molten_counts,
    mapping=ggplot2::aes(x=Gene_Expression, color=Sample_id),
) + ggplot2::geom_density() + ggplot2::xlim(0, 3000)
```

Are these distribution "normal" ?

<details>

<summary>Hints</summary>

To help you discuss, I have drawn normal distrubitions
with means and standard deviations equal to the means and
standard deviations of each sample.

I have used the same method as earlier to building the plot.
I have used the function [`rnorm`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Normal)
from package [`stats`](https://www.rdocumentation.org/packages/stats/versions/3.6.2)
to build the distributions, and the values present in the summary
to fill their arguments. These distribution were stored
in a data frame, using the function [`data.frame`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame)
from [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/) package.
Since we cannot have negative counts values, I used the
function [`pmax`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Extremes)
from [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Extremes) package.

```{r create_and_display_normal_distributions}
normal_distributions <- reshape2::melt(
    base::data.frame(
        Day_0_R1=base::pmax(stats::rnorm(n=25275, mean=2311, sd=7708), 0),
        Day_0_R2=base::pmax(stats::rnorm(n=25275, mean=2021, sd=6802), 0),
        Day_0_R3=base::pmax(stats::rnorm(n=25275, mean=1914, sd=6390), 0),
        Day_7_R1=base::pmax(stats::rnorm(n=25275, mean=1666, sd=5480), 0),
        Day_7_R2=base::pmax(stats::rnorm(n=25275, mean=1704, sd=5628), 0),
        Day_7_R3=base::pmax(stats::rnorm(n=25275, mean=1792, sd=5938), 0)
    ),
    variable.name="Sample_id",
    value.name="Gene_Expression"
)
ggplot2::ggplot(
    data=normal_distributions,
    mapping=ggplot2::aes(x=Gene_Expression, color=Sample_id),
) + ggplot2::geom_density()
```

</details>
<br />


With the over-representation of the zeros and very low values,
as well as the absence of negative values, we cannot *feel* if
the distribution is normal or not.

We can see this is not binomial, but nothing more.

## Run normality test

There is a statistical test that lets us assess the odds of our
distribution to be likely normal. That tests is available with
the function [`shapiro.test`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test)
and is available in the package [`stats`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/).
However, the Shapiro test expects *at most* 5 000
observations. This means we would have to take
5 000 random genes and use them in the test.
Another solution is to try the Anderson-Darling test,
which is know to be a little less significative, but
which can handle our complete dataset.

That function expects a vector of numbers, not a
data frame. We have to apply that function to all
columns of our data frame,]
using the function [`apply`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply)
from the package [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/).

```{r apply_shapiro_on_df}
# Run shapiro on the 5000 first counts
base::apply(counts[1:5000, ], 2, stats::shapiro.test)

# Run the Anderson-Darling test on the complete gene list
base::library(package="nortest", character.only=TRUE)
base::apply(counts, 2, nortest::ad.test)
```

Both tests agree: the counts in this table are very likely to follow a normal distribution.


## Scaling factor

The mean and standard deviations are alike, the expression follows a normal
distribution, our final check to know if we can compare the samples, is to be
sure we have the same amount of counts in each sides.

Otherwise, we cannot know the abundance of a gene count.

Imagine a sudent with 5/5 at their exam, and another with 5/20.
They do not have the same grade, even if they have the same score.

This is the same with gene expression. Let's compute the
sum of the gene expression using [`colSums`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/colSums)
from the package [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/colSums).

```{r sample_expression_sum}
gene_sums <- base::colSums(x=counts, na.rm=TRUE)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_colsums, eval=FALSE}
utils::head(x=gene_sums)
```

```{r NR_kittr_colsums, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=gene_sums),
  caption = "Per sample sum of gene expression"
)
```

So, since the gene *ENO1* has an expression of 186 201 over 58 413 055 in the 
sample Day_0_R1, is it more expressed that the same gene in Day_0_R2, knowing 
its abundance is 176 277 over 51 090 061 ?

Maybe yes...? We can't be sure.

## Conclusion

I can assure you, these data were **not** normalized at all. This was an illustration
of why one cannot be sure of the nature of the count table they have in their hands

# Normalize data

## TPM vs RPKM vs RPM vs others

[RNASeq-blog](https://www.rna-seqblog.com/) have a very nice article about
normaliztions: [RPKM, FPKM and TPM, clearly explained](https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/)

The main thing to remember:

1. Never, ever use RPKM. There are no situation where this would be profitable. It has been prooven wrong since 2013, and abandonned since 2015.
1. When considering ChIP-Seq data, with a single-ended library, FPKM and RPKM have the same values.
1. TPM and RPM should always be preferred when you work on exploratory quantification dataset. In other cases (differential expression, etc), other metods can perofrm better.
1. Never compare counts *before* normalization.
1. Never compare counts normalized with different methods.
1. One can always normalize with TPM/RPM and find the original counts back.

## Applying TPM normalization

### Get gene length

To normalize using TPM, we need the gene lengths. The package
[`biomaRt`](https://rdrr.io/bioc/biomart/) has the following set
of functions used to retrieve this information:

```{r get_gene_lengths}
base::library(package="biomaRt", character.only=TRUE)

# Retrieve database information
human <- biomaRt::useMart("ensembl", dataset="hsapiens_gene_ensembl")
gene_lengths <- biomaRt::getBM(
    attributes=c("hgnc_symbol","ensembl_gene_id", "start_position","end_position"),
    filters="hgnc_symbol",
    values=base::rownames(counts),
    mart=human
)

# Compute gene lengths
gene_lengths$size <- gene_lengths$end_position - gene_lengths$start_position
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_gene_length, eval=FALSE}
utils::head(x=gene_lengths)
```

```{r NR_knitr_gene_length, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=gene_lengths),
  caption = "Per sample sum of gene expression"
)
```

### Compute TPM

https://rdrr.io/github/AmyOlex/RNASeqBits/man/calc.tpm.html
We can use the function [`calculateTPM`](https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateTPM)
from package [`scater`](https://www.rdocumentation.org/packages/scater/versions/1.0.4)
to easily compute TPM values based on gene length.

```{r scater_tpm}
base::library(package="scater", character.only=TRUE)
tpm <- scater::calcumateTPM(
    object=
```

### Compute RPK

We can now compute RPK, using the function
[`sweep`](https://rdocumentation.org/packages/base/versions/3.6.2/topics/sweep)
from the package [`base`](https://rdocumentation.org/packages/base/versions/3.6.2/).

```{r compute_rpk}
rpk <- base::sweep(
    x=sorted_counts,
    MARGIN=2,
    STATS=gene_lengths$size,
    FUN="/"
)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_rpk, eval=FALSE}
utils::head(x=rpk)
```

```{r NR_knitr_rpk, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=rpk),
  caption = "Read per kilobase"
)
```

### Compute per milion scaling factor

We already computed library size earlier, and sotred it into
a variable named `gene_sums`. The same method is used to
sum up all the reads per kilobase.

The scaling factor is obtained by dividing that number
by 1 milion.

```{r per_mil_scaling_factor}
scaling_factor <- base::colSums(rpk) / 1000000
base::print(scaling_factor)
```

### Get TPM

Finally, we divide the RPK by the scaling factor:

```{r divide_rpk_scaling_factor}
tpm <- rpk / scaling_factor
base::colSums(tpm)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_tpm, eval=FALSE}
utils::head(x=tpm)
```

```{r NR_knitr_tpm, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=tpm),
  caption = "TPM normalized counts"
)
```

## Conclusions

More about normalizations [on the web](https://www.reneshbedre.com/blog/expression_units.html).

# Filtering

Removing rows filled with zeros is a common practice.
While this does not bother differential analysis, or
quality controls, these rows full of zeros take require
memory to be stored, and take time to compute.

Let's remove them. We can use the function [`dim`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/dim)
from package [`base`](https://www.rdocumentation.org/packages/base/versions/3.6.2)
to check how many rows vere filtered out.

```{r remove_full_zeros}
base::dim(x=tpm)
tpm <- tpm[base::rowSums(tpm[]) > 0, ]
base::dim(x=tpm)
```

Then we check the results with the function
[`head`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head)
from the [`utils`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/)
package.

```{r head_tpm_no_zero, eval=FALSE}
utils::head(x=tpm)
```

```{r NR_knitr_tpm_no_zero, echo=FALSE, results='asis'}
knitr::kable(
  x = utils::head(x=tpm),
  caption = "TPM normalized counts"
)
```

# Graphs

## Plot sample correlation heatmap

```{r compute_corr_matrix}
#corr_matrix <- cor(data_normalized)
```

## Plot PCA (scatterplot)

# Save filtered table

## Save table as CSV/TSV

## Save table as Excel

## Save table as RDS

